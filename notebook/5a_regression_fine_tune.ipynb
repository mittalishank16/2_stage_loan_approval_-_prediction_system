{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "918a5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import probplot\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, ExtraTreesRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "#from pandas_profiling import ProfileReport\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "import optuna\n",
    "import mlflow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8dda41",
   "metadata": {},
   "source": [
    "from **4a_regression_train.ipynb** we know: \n",
    "\n",
    "### ** Top 5  Models **\n",
    "\n",
    "| Rank | Model | Mean Score | Standard Deviation |\n",
    "|:---|:---|:---|:---|\n",
    "| 1 | **Gradient Boosting Regression** | 3871.92 | 45.50 |\n",
    "| 2 | **Random Forest Regression** | 4026.38 | 77.62 |\n",
    "| 3 | **Extra Trees Regression** | 4090.38 | 61.13 |\n",
    "| 4 | **XG Boost** | 4130.37 | 37.57 |\n",
    "| 5 | **Bagging Regression** | 4207.16 | 109.43 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2d7f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original = pd.read_csv(r'C:\\Users\\user\\Desktop\\ML & DL projects\\2 Stage loan system\\data\\feature_engineered\\train_regression_data.csv')\n",
    "test_original = pd.read_csv(r'C:\\Users\\user\\Desktop\\ML & DL projects\\2 Stage loan system\\data\\feature_engineered\\test_regression_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a375db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_original.drop(columns='Loan Sanction Amount (USD)', axis = 1)\n",
    "y_train = train_original['Loan Sanction Amount (USD)']\n",
    "\n",
    "\n",
    "X_test = test_original.drop(columns='Loan Sanction Amount (USD)', axis = 1)\n",
    "y_test = test_original['Loan Sanction Amount (USD)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9f016",
   "metadata": {},
   "source": [
    "### Fine Tuning the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "893cc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # 1. Let Optuna pick which model to tune in this trial\n",
    "    model_type = trial.suggest_categorical(\"model_type\", [\"GradientBoosting\", \"RandomForest\", \"ExtraTrees\", \"XGBoost\"])\n",
    "    \n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        \n",
    "        if model_type == \"GradientBoosting\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"gb_n_estimators\", 100, 1000),\n",
    "                \"learning_rate\": trial.suggest_float(\"gb_lr\", 0.01, 0.2, log=True),\n",
    "                \"max_depth\": trial.suggest_int(\"gb_max_depth\", 3, 10),\n",
    "                \"subsample\": trial.suggest_float(\"gb_subsample\", 0.7, 1.0),\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "            model = GradientBoostingRegressor(**params)\n",
    "\n",
    "        elif model_type == \"RandomForest\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"rf_n_estimators\", 100, 1000),\n",
    "                \"max_depth\": trial.suggest_int(\"rf_max_depth\", 10, 50),\n",
    "                \"min_samples_split\": trial.suggest_int(\"rf_split\", 2, 10),\n",
    "                \"max_features\": trial.suggest_categorical(\"rf_feat\", [\"sqrt\", \"log2\", None]),\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": -1\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "\n",
    "        elif model_type == \"ExtraTrees\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"et_n_estimators\", 100, 1000),\n",
    "                \"max_depth\": trial.suggest_int(\"et_max_depth\", 10, 50),\n",
    "                \"min_samples_split\": trial.suggest_int(\"et_split\", 2, 10),\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": -1\n",
    "            }\n",
    "            model = ExtraTreesRegressor(**params)\n",
    "\n",
    "        elif model_type == \"XGBoost\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 100, 1000),\n",
    "                \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_float(\"xgb_lr\", 0.01, 0.2, log=True),\n",
    "                \"lambda\": trial.suggest_float(\"xgb_lambda\", 1e-3, 10.0, log=True),\n",
    "                \"alpha\": trial.suggest_float(\"xgb_alpha\", 1e-3, 10.0, log=True),\n",
    "                \"random_state\": 42,\n",
    "                \"tree_method\": \"hist\"\n",
    "            }\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "\n",
    "        # 2. Train and Evaluate\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        mae = float(mean_absolute_error(y_test, y_pred))\n",
    "        r2 = float(r2_score(y_test, y_pred))\n",
    "\n",
    "        # 3. Log results\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics({\"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67146ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 01:39:09,780] A new study created in memory with name: no-name-b7f2c6a6-2d6f-42f9-b43e-42d2e9838e0d\n",
      "[I 2026-02-02 01:39:32,347] Trial 0 finished with value: 10757.801486288949 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 810, 'gb_lr': 0.08587756561441867, 'gb_max_depth': 5, 'gb_subsample': 0.8319273719964079}. Best is trial 0 with value: 10757.801486288949.\n",
      "[I 2026-02-02 01:39:32,646] Trial 1 finished with value: 10948.204509609302 and parameters: {'model_type': 'ExtraTrees', 'et_n_estimators': 153, 'et_max_depth': 10, 'et_split': 7}. Best is trial 0 with value: 10757.801486288949.\n",
      "[I 2026-02-02 01:39:53,746] Trial 2 finished with value: 10483.918086285425 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 926, 'gb_lr': 0.03476662207269958, 'gb_max_depth': 4, 'gb_subsample': 0.8356773422314203}. Best is trial 2 with value: 10483.918086285425.\n",
      "[I 2026-02-02 01:39:54,732] Trial 3 finished with value: 10787.188709371883 and parameters: {'model_type': 'XGBoost', 'xgb_n_estimators': 293, 'xgb_max_depth': 10, 'xgb_lr': 0.03181213234663415, 'xgb_lambda': 0.11303222228930958, 'xgb_alpha': 0.004415054492882129}. Best is trial 2 with value: 10483.918086285425.\n",
      "[I 2026-02-02 01:39:54,939] Trial 4 finished with value: 10438.788130301522 and parameters: {'model_type': 'XGBoost', 'xgb_n_estimators': 348, 'xgb_max_depth': 3, 'xgb_lr': 0.09254098808726936, 'xgb_lambda': 0.0205766849058344, 'xgb_alpha': 1.0428717843754216}. Best is trial 4 with value: 10438.788130301522.\n",
      "[I 2026-02-02 01:40:09,223] Trial 5 finished with value: 10451.734230848808 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 433, 'gb_lr': 0.03405396747482842, 'gb_max_depth': 7, 'gb_subsample': 0.7058165152395419}. Best is trial 4 with value: 10438.788130301522.\n",
      "[I 2026-02-02 01:40:14,790] Trial 6 finished with value: 10391.81934558625 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 269, 'gb_lr': 0.012862545147692401, 'gb_max_depth': 4, 'gb_subsample': 0.7447382323673253}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:15,563] Trial 7 finished with value: 10433.344754849033 and parameters: {'model_type': 'ExtraTrees', 'et_n_estimators': 371, 'et_max_depth': 13, 'et_split': 3}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:17,408] Trial 8 finished with value: 10809.25242033467 and parameters: {'model_type': 'RandomForest', 'rf_n_estimators': 891, 'rf_max_depth': 26, 'rf_split': 2, 'rf_feat': 'sqrt'}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:18,128] Trial 9 finished with value: 10718.738202177621 and parameters: {'model_type': 'XGBoost', 'xgb_n_estimators': 762, 'xgb_max_depth': 7, 'xgb_lr': 0.05147795229096199, 'xgb_lambda': 0.4933259768661216, 'xgb_alpha': 0.004611387242196181}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:18,741] Trial 10 finished with value: 10489.473089052151 and parameters: {'model_type': 'RandomForest', 'rf_n_estimators': 100, 'rf_max_depth': 50, 'rf_split': 10, 'rf_feat': None}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:21,324] Trial 11 finished with value: 10478.163550855576 and parameters: {'model_type': 'ExtraTrees', 'et_n_estimators': 599, 'et_max_depth': 22, 'et_split': 2}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:22,619] Trial 12 finished with value: 10533.915193039931 and parameters: {'model_type': 'ExtraTrees', 'et_n_estimators': 238, 'et_max_depth': 50, 'et_split': 2}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:29,630] Trial 13 finished with value: 11480.24859930399 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 141, 'gb_lr': 0.013942228524272451, 'gb_max_depth': 10, 'gb_subsample': 0.7157512466643366}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:30,996] Trial 14 finished with value: 10598.872120255004 and parameters: {'model_type': 'ExtraTrees', 'et_n_estimators': 905, 'et_max_depth': 11, 'et_split': 10}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:32,557] Trial 15 finished with value: 10511.497759904853 and parameters: {'model_type': 'ExtraTrees', 'et_n_estimators': 488, 'et_max_depth': 34, 'et_split': 5}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:36,663] Trial 16 finished with value: 11456.100122716161 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 198, 'gb_lr': 0.011133314900904039, 'gb_max_depth': 3, 'gb_subsample': 0.9768345133694422}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:37,243] Trial 17 finished with value: 11547.300683336909 and parameters: {'model_type': 'RandomForest', 'rf_n_estimators': 390, 'rf_max_depth': 10, 'rf_split': 6, 'rf_feat': 'log2'}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:54,550] Trial 18 finished with value: 11267.418124484253 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 449, 'gb_lr': 0.18652595533758512, 'gb_max_depth': 7, 'gb_subsample': 0.7835286776631314}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:40:56,151] Trial 19 finished with value: 10511.57075036569 and parameters: {'model_type': 'ExtraTrees', 'et_n_estimators': 474, 'et_max_depth': 25, 'et_split': 5}. Best is trial 6 with value: 10391.81934558625.\n",
      "[I 2026-02-02 01:41:16,383] Trial 20 finished with value: 10340.755595962333 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 631, 'gb_lr': 0.018243247603923353, 'gb_max_depth': 5, 'gb_subsample': 0.9534551057921441}. Best is trial 20 with value: 10340.755595962333.\n",
      "[I 2026-02-02 01:41:37,449] Trial 21 finished with value: 10331.066787549593 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 649, 'gb_lr': 0.019032343956551134, 'gb_max_depth': 5, 'gb_subsample': 0.9640986543542458}. Best is trial 21 with value: 10331.066787549593.\n",
      "[I 2026-02-02 01:41:59,304] Trial 22 finished with value: 10323.348150233824 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 669, 'gb_lr': 0.017519003136725667, 'gb_max_depth': 5, 'gb_subsample': 0.9704653554531949}. Best is trial 22 with value: 10323.348150233824.\n",
      "[I 2026-02-02 01:42:24,945] Trial 23 finished with value: 10359.490851790066 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 671, 'gb_lr': 0.022152120246059923, 'gb_max_depth': 6, 'gb_subsample': 0.9742876178576548}. Best is trial 22 with value: 10323.348150233824.\n",
      "[I 2026-02-02 01:42:44,415] Trial 24 finished with value: 10327.701765242044 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 637, 'gb_lr': 0.020459449538936577, 'gb_max_depth': 5, 'gb_subsample': 0.9113590329379002}. Best is trial 22 with value: 10323.348150233824.\n",
      "[I 2026-02-02 01:43:11,514] Trial 25 finished with value: 10358.865740042756 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 744, 'gb_lr': 0.022199124739578784, 'gb_max_depth': 6, 'gb_subsample': 0.91472333823883}. Best is trial 22 with value: 10323.348150233824.\n",
      "[I 2026-02-02 01:43:27,638] Trial 26 finished with value: 10451.947198281854 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 534, 'gb_lr': 0.056598838244904554, 'gb_max_depth': 5, 'gb_subsample': 0.901930855564798}. Best is trial 22 with value: 10323.348150233824.\n",
      "[I 2026-02-02 01:44:08,219] Trial 27 finished with value: 10541.722906122453 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 845, 'gb_lr': 0.025325131528226397, 'gb_max_depth': 8, 'gb_subsample': 0.9182769226787127}. Best is trial 22 with value: 10323.348150233824.\n",
      "[I 2026-02-02 01:44:24,952] Trial 28 finished with value: 10299.361430297395 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 630, 'gb_lr': 0.016833112938731944, 'gb_max_depth': 4, 'gb_subsample': 0.9916956598760839}. Best is trial 28 with value: 10299.361430297395.\n",
      "[I 2026-02-02 01:44:34,734] Trial 29 finished with value: 10360.061100746607 and parameters: {'model_type': 'GradientBoosting', 'gb_n_estimators': 529, 'gb_lr': 0.010050220065239774, 'gb_max_depth': 3, 'gb_subsample': 0.8839713238713528}. Best is trial 28 with value: 10299.361430297395.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "OPTIMIZATION COMPLETE\n",
      "==============================\n",
      "Best Model Type: GradientBoosting\n",
      "Best RMSE Score: 10299.3614\n",
      "------------------------------\n",
      "Best Hyperparameters:\n",
      "  model_type: GradientBoosting\n",
      "  gb_n_estimators: 630\n",
      "  gb_lr: 0.016833112938731944\n",
      "  gb_max_depth: 4\n",
      "  gb_subsample: 0.9916956598760839\n"
     ]
    }
   ],
   "source": [
    "# 1. Set the tracking URI to your specific project folder\n",
    "mlflow.set_tracking_uri(r\"file:///C:\\Users\\user\\Desktop\\ML & DL projects\\2 Stage loan system\\mlruns\")\n",
    "\n",
    "# 2. Rename the experiment to reflect the Top 4 model competition\n",
    "mlflow.set_experiment(\"top_4_models_optimization\")\n",
    "\n",
    "# 3. Create and run the study\n",
    "# We use 'minimize' because we are returning RMSE from the objective function\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)  # Increased trials to give each model a fair chance\n",
    "\n",
    "# 4. Results Interpretation\n",
    "print(\"=\"*30)\n",
    "print(\"OPTIMIZATION COMPLETE\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Best Model Type: {study.best_params['model_type']}\")\n",
    "print(f\"Best RMSE Score: {study.best_value:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bef1eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/02 01:44:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Tuned Model (GradientBoosting) Performance:\n",
      "MAE: 5836.3319\n",
      "RMSE: 10311.0137\n",
      "R²: 0.9166\n"
     ]
    }
   ],
   "source": [
    "# 1. Retrieve best parameters\n",
    "best_params = study.best_params.copy()\n",
    "model_type = best_params.pop(\"model_type\")\n",
    "\n",
    "# 2. Dynamic Initialization with Name Mapping\n",
    "match model_type:\n",
    "    case \"GradientBoosting\":\n",
    "        # Map Optuna keys to official sklearn names\n",
    "        mapping = {\"gb_lr\": \"learning_rate\", \"gb_n_estimators\": \"n_estimators\", \n",
    "                   \"gb_max_depth\": \"max_depth\", \"gb_subsample\": \"subsample\"}\n",
    "        clean_params = {mapping.get(k, k): v for k, v in best_params.items()}\n",
    "        best_model = GradientBoostingRegressor(**clean_params)\n",
    "        \n",
    "    case \"RandomForest\":\n",
    "        mapping = {\"rf_n_estimators\": \"n_estimators\", \"rf_max_depth\": \"max_depth\", \n",
    "                   \"rf_split\": \"min_samples_split\", \"rf_feat\": \"max_features\"}\n",
    "        clean_params = {mapping.get(k, k): v for k, v in best_params.items()}\n",
    "        best_model = RandomForestRegressor(**clean_params, n_jobs=-1)\n",
    "        \n",
    "    case \"ExtraTrees\":\n",
    "        mapping = {\"et_n_estimators\": \"n_estimators\", \"et_max_depth\": \"max_depth\", \n",
    "                   \"et_split\": \"min_samples_split\"}\n",
    "        clean_params = {mapping.get(k, k): v for k, v in best_params.items()}\n",
    "        best_model = ExtraTreesRegressor(**clean_params, n_jobs=-1)\n",
    "        \n",
    "    case \"XGBoost\":\n",
    "        mapping = {\"xgb_lr\": \"learning_rate\", \"xgb_n_estimators\": \"n_estimators\", \n",
    "                   \"xgb_max_depth\": \"max_depth\", \"xgb_lambda\": \"reg_lambda\", \"xgb_alpha\": \"reg_alpha\"}\n",
    "        clean_params = {mapping.get(k, k): v for k, v in best_params.items()}\n",
    "        best_model = xgb.XGBRegressor(**clean_params)\n",
    "        \n",
    "    case _:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "# 3. Train and 4. Evaluate (rest of your code remains the same)\n",
    "best_model.fit(X_train, y_train)\n",
    "# 4. Evaluate on the evaluation set\n",
    "y_pred = best_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Final Tuned Model ({model_type}) Performance:\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# 5. Log the final run to MLflow\n",
    "with mlflow.start_run(run_name=f\"best_{model_type.lower()}_final\"):\n",
    "    mlflow.log_params(clean_params)\n",
    "    mlflow.log_metrics({\"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "    \n",
    "    # Use the appropriate flavor for logging\n",
    "    if model_type == \"XGBoost\":\n",
    "        mlflow.xgboost.log_model(best_model, artifact_path=\"model\")\n",
    "    else:\n",
    "        mlflow.sklearn.log_model(best_model, artifact_path=\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df5e60d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved locally as best_gradientboosting_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# 6. Save the model locally as a backup\n",
    "joblib.dump(best_model, f\"best_{model_type.lower()}_model.pkl\")\n",
    "print(f\"Model saved locally as best_{model_type.lower()}_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
